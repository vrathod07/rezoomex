# -*- coding: utf-8 -*-
"""Concept-Extraction Module.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aDnc-Lslp327n6TyG1opjLn2nWIAhMah
"""

def init_glove_vectors():
  #!wget http://nlp.stanford.edu/data/glove.6B.zip
  #!unzip glove.6B.zip
  #!ls -lrt
  return


def conceptio_results(ans):
  from bs4 import BeautifulSoup
  import requests
  # ans = input("Insert the word to be searched-")
  url = "https://conceptnet.io/c/en/"
  for letter in ans:
     if (letter.isalpha()):
        url = url + letter.lower()
  print(url)
  page = requests.get(url)
  print(page)
  soup = BeautifulSoup(page.content,'html.parser')
  lists = soup.find_all('div', class_="pure-u-1 pure-u-md-1-2 pure-u-lg-1-3 pure-u-xl-1-4 feature-box")
  terms = []
  for list in lists:
     term = list.find('li',class_="term lang-en")
     if (term):
         terms.append(term)
  concept_words = []
  result = []
  words = []
  for term in terms:
    final = term.find('a').text
    final1 = final[13:]
    final2 = final1[:-9]
    result.append(final2)
    final3 = final2.split(' ')
    final4 = final3[-1]
    #print(len(final2))
    words.append(final4)
  n = len(words)//3
  for i in range(n):
    concept_words.append(words[i])
  print(concept_words)
  return [concept_words, result]

def distances(word1, word2):
  import os
  import numpy as np
  word2vector = {}

  with open('orphan-entity-allocation\WSD\data\glove.6B\glove.6B.50d.txt') as file:
  
    for line in file:
       list_of_values = line.split()    	
       word = list_of_values[0]
       vector_of_word = np.asarray(list_of_values[1:], dtype='float32')
       word2vector[word] = vector_of_word
  if (str(type(word1)) != "<class 'str'>" or str(type(word2)) != "<class 'str'>"):
    return 10000
  else :
   dist = np.linalg.norm(word2vector.get(word1)- word2vector.get(word2)) #range of dist?
   return dist

def is_special(text):
    rem=' '
    for i in text :
        if i.isalpha():
            rem=rem+i
        elif i.isdigit():
            rem=rem+i
        else:
            rem=rem+' '
    return rem

def lowercase(text):
    return text.lower()

def remove_stopwords(text):
    import nltk 
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
    #from google.colab import files
    from nltk.stem import WordNetLemmatizer
    #from wordcloud import WordCloud, STOPWORDS
    nltk.download('stopwords')
    nltk.download('punkt')
    stop_words=set(stopwords.words('english'))
    words=word_tokenize(text)
    return [w for w in words if w not in stop_words]

def context(oe):
  import pandas as pd
  import numpy as np
  import re
  import nltk 
  from nltk.tokenize import word_tokenize
  from nltk.corpus import stopwords
  #from google.colab import files
  from nltk.stem import WordNetLemmatizer
  #from wordcloud import WordCloud, STOPWORDS
  nltk.download('stopwords')
  nltk.download('punkt')
  import io
  #uploaded = files.upload() #keep the name of the file Resumes.csv always
  df = pd.read_csv('orphan-entity-allocation\Integration\data\Resumes.csv')
  df.head()
  df.Resumes = df.Resumes.apply(is_special)
  df.Resumes = df.Resumes.apply(lowercase)
  df.Resumes = df.Resumes.apply(remove_stopwords)
  df.head()
  related = df.Resumes[1]
  related_set = (set(related))
  context_words = []
  for ele in related_set:
    context_words.append(ele)
  if(oe in context_words):
    context_words.remove(oe)
  #related.remove(oe)
  #print(related)
  return context_words

def average_dist(context_words, concept_words):
  n = len(context_words)
  distance = []
  for i in concept_words:
    dist = 0
    for j in context_words:
      dist += distances(i,j)
    dist /= n
    distance.append(dist)
  print(distance)
  return distance

def final(oe, context_words, concept_words, result):
  distances = average_dist(context_words, concept_words)
  ans = min(distances)
  answer = result[distances.index(ans)]
  finalr =[]
  finalr.append(oe)
  finalr.append("is")
  finalr.append(answer)
  print(finalr)
  return finalr

def concept_extraction(oe):
  init_glove_vectors()
  context_words = context(oe)
  temp = conceptio_results(oe)
  concept_words = temp[0]
  result = temp[1]
  final(oe, context_words, concept_words, result)

# if __name__ == "__main__":
#     oe = 'scikit-learn'
#     concept_extraction(oe[0])

